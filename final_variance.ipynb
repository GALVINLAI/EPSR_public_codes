{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "from scipy.optimize import minimize\n",
    "import scipy.linalg\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "from scipy.optimize import differential_evolution, basinhopping, shgo\n",
    "\n",
    "from epsr import EPSR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended parameter shift rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.48803387 -0.33333333  0.17863279]\n",
      "[-1.5  2.  -0.5]\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "d=1\n",
    "r=3\n",
    "Omegas=np.arange(1, r + 1)\n",
    "x = np.array([(2 * mu - 1) / (2 * r) * np.pi for mu in range(1, r + 1)])\n",
    "# x = np.random.uniform(0, np.pi, r)\n",
    "b=EPSR(x, Omegas, d)\n",
    "print(b)\n",
    "\n",
    "d=2\n",
    "r=2\n",
    "Omegas=np.arange(1, r + 1)\n",
    "x = np.array([ mu * (np.pi / r)  for mu in range(0, r + 1)])\n",
    "# x = np.random.uniform(0, np.pi, r+1)\n",
    "b=EPSR(x, Omegas, d)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2norm2FinalVar(x, Omegas, d):\n",
    "    b=EPSR(x, Omegas, d)\n",
    "    return 0.5*np.sum(np.abs(b)**2)\n",
    "\n",
    "def L2normFinalVar(x, Omegas, d):\n",
    "    b=EPSR(x, Omegas, d)\n",
    "    return np.sqrt(np.sum(np.abs(b)**2))\n",
    "\n",
    "def L1normFinalVar(x, Omegas, d):\n",
    "    b=EPSR(x, Omegas, d)\n",
    "    return np.sum(np.abs(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**EXP 1:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal x_mu: [0.78539843 2.35619583]\n",
      "Minimized l1 norm: 2.0000000000012914\n",
      "equadistant x_mu: [0.78539816 2.35619449]\n",
      "l1 norm when using equadistantx_mu: 2.0\n",
      "[ True  True]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# r = 1\n",
    "r = 2\n",
    "# r = 3\n",
    "# r = 4\n",
    "# r = 5\n",
    "# r = 6\n",
    "# r = 7\n",
    "\n",
    "# odd order d\n",
    "d = 1\n",
    "# d = 3\n",
    "# d = 5\n",
    "\n",
    "Omegas = np.arange(1, r+1)\n",
    "\n",
    "# L2norm2_objective = lambda x:  L2norm2FinalVar(x, Omegas, d)\n",
    "# L2norm_objective = lambda x:  L2normFinalVar(x, Omegas, d)\n",
    "L1norm_objective = lambda x:  L1normFinalVar(x, Omegas, d)\n",
    "\n",
    "# Initial guess for x_mu\n",
    "x_mu = np.array([(2 * mu - 1) / (2 * r) * np.pi for mu in range(1, r + 1)])\n",
    "# x_mu_initial = np.random.uniform(0,  np.pi, r)\n",
    "\n",
    "# Perform optimization\n",
    "# result = minimize(objective, x_mu_initial, method='BFGS')\n",
    "# result = basinhopping(objective, x_mu_initial, niter=100)\n",
    "\n",
    "# Define bounds for optimization (0 to pi for each variable)\n",
    "bounds = [(0.01, np.pi-0.01) for _ in range(r)]\n",
    "\n",
    "result = differential_evolution(L1norm_objective, bounds)\n",
    "# Perform optimization with bounds\n",
    "# result = minimize(objective, x_mu_initial, method='SLSQP', bounds=bounds)\n",
    "\n",
    "# Optimal x_mu and corresponding minimized l1 norm\n",
    "optimal_x_mu = result.x\n",
    "optimal_x_mu.sort()\n",
    "minimized_l1norm = result.fun\n",
    "\n",
    "# Output the results\n",
    "print(\"Optimal x_mu:\", optimal_x_mu)\n",
    "print(\"Minimized l1 norm:\", minimized_l1norm)\n",
    "print(\"equadistant x_mu:\", x_mu)\n",
    "print(\"l1 norm when using equadistantx_mu:\", L1norm_objective(x_mu))\n",
    "print(np.isclose(optimal_x_mu, x_mu))\n",
    "print(np.isclose(minimized_l1norm, L1norm_objective(x_mu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**EXP 2:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal x_mu: [0.         1.57079632 3.14159241]\n",
      "Minimized l1 norm: 64.00000000000188\n",
      "equadistant x_mu: [0.         1.57079633 3.14159265]\n",
      "l1 norm when using equadistantx_mu: 64.0\n",
      "[ True  True  True]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# r = 1\n",
    "r = 2\n",
    "# r = 3\n",
    "# r = 4\n",
    "# r = 5\n",
    "# r = 6\n",
    "# r = 7\n",
    "\n",
    "# even order d\n",
    "# d = 2\n",
    "# d = 4\n",
    "d = 6\n",
    "\n",
    "Omegas = np.arange(1, r+1)\n",
    "\n",
    "L1norm_objective = lambda x:  L1normFinalVar(x, Omegas, d)\n",
    "\n",
    "# Initial guess for x_mu\n",
    "x_mu = np.array([ mu * (np.pi / r)  for mu in range(0, r + 1)])\n",
    "\n",
    "# Define bounds for optimization (0 to pi for each variable)\n",
    "bounds = [(0, np.pi) for _ in range(r+1)]\n",
    "# x_mu_initial = np.random.uniform(0,  np.pi, r)\n",
    "\n",
    "result = differential_evolution(L1norm_objective, bounds)\n",
    "# Perform optimization with bounds\n",
    "# result = minimize(objective, x_mu_initial, method='SLSQP', bounds=bounds)\n",
    "\n",
    "# Optimal x_mu and corresponding minimized l1 norm\n",
    "optimal_x_mu = result.x\n",
    "optimal_x_mu.sort()\n",
    "minimized_l1norm = result.fun\n",
    "\n",
    "# Output the results\n",
    "print(\"Optimal x_mu:\", optimal_x_mu)\n",
    "print(\"Minimized l1 norm:\", minimized_l1norm)\n",
    "print(\"equadistant x_mu:\", x_mu)\n",
    "print(\"l1 norm when using equadistantx_mu:\", L1norm_objective(x_mu))\n",
    "print(np.isclose(optimal_x_mu, x_mu))\n",
    "print(np.isclose(minimized_l1norm, L1norm_objective(x_mu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXP 3: l1 norm_objective sugradient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.26325641e-13, -2.27373675e-13, -5.68434189e-13, -9.09494702e-13,\n",
       "       -8.52651283e-13])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r = 1\n",
    "# r = 2\n",
    "# r = 3\n",
    "# r = 4\n",
    "r = 5\n",
    "# r = 6\n",
    "# r = 7\n",
    "\n",
    "# odd order d\n",
    "d = 1\n",
    "# d = 3\n",
    "d = 5\n",
    "\n",
    "Omegas = np.arange(1, r + 1)\n",
    "\n",
    "# L2norm2_objective = lambda x: L2norm2FinalVar(x, Omegas, d)\n",
    "# L2norm_objective = lambda x: L2normFinalVar(x, Omegas, d)\n",
    "L1norm_objective = lambda x: L1normFinalVar(x, Omegas, d)\n",
    "\n",
    "# Define the analytical subgradient function (just one of the subgradients)\n",
    "def analytical_subgradient_l1_odd(x, Omegas):\n",
    "    p = Omegas ** d * (-1 if d % 4 == 3 else 1)\n",
    "    A = np.sin(np.outer(x, Omegas))\n",
    "    A_1 = np.cos(np.outer(x, Omegas)) * Omegas\n",
    "    A_inv = np.linalg.inv(A)\n",
    "    b = np.linalg.inv(A.T) @ p\n",
    "    subgrad = - np.diag(A_1 @ A_inv @ np.outer(np.sign(b), b))\n",
    "    return subgrad\n",
    "\n",
    "x_mu = np.array([(2 * mu - 1) / (2 * r) * np.pi for mu in range(1, r + 1)])\n",
    "# x_mu = np.random.uniform(0, np.pi, size=r)\n",
    "analytical_subgradient_l1_odd(x_mu, Omegas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00000000e+00, -3.09228199e-11, -4.18367563e-11, -2.81943358e-11,\n",
       "       -3.18323146e-11, -4.18367563e-11, -3.63797881e-11, -2.08856776e-11])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 1\n",
    "r = 2\n",
    "r = 3\n",
    "r = 4\n",
    "r = 5\n",
    "r = 6\n",
    "r = 7\n",
    "\n",
    "# even order d\n",
    "d = 2\n",
    "d = 4\n",
    "d = 6\n",
    "\n",
    "Omegas = np.arange(1, r + 1)\n",
    "\n",
    "# L2norm2_objective = lambda x: L2norm2FinalVar(x, Omegas, d)\n",
    "# L2norm_objective = lambda x: L2normFinalVar(x, Omegas, d)\n",
    "L1norm_objective = lambda x: L1normFinalVar(x, Omegas, d)\n",
    "\n",
    "# Define the analytical subgradient function (just one of the subgradients)\n",
    "def analytical_subgradient_l1_even(x, Omegas):\n",
    "    q = Omegas ** d * (-1 if d % 4 == 2 else 1)\n",
    "    q = np.insert(q, 0, 1 if d == 0 else 0)\n",
    "    Omegas = np.insert(Omegas, 0, 0)\n",
    "    A = np.cos(np.outer(x, Omegas))\n",
    "    A_1 = -np.sin(np.outer(x, Omegas)) * Omegas\n",
    "    A_inv = np.linalg.inv(A)\n",
    "    b = np.linalg.inv(A.T) @ q\n",
    "    subgrad = - np.diag(A_1 @ A_inv @ np.outer(np.sign(b), b))\n",
    "    return subgrad\n",
    "\n",
    "x_mu = np.array([mu * (np.pi / r) for mu in range(0, r + 1)])\n",
    "# x_mu = np.random.uniform(0, np.pi, size=r+1)\n",
    "analytical_subgradient_l1_even(x_mu, Omegas)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
